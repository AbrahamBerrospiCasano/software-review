---
title: "Another Year in Reviews at rOpenSci: DRAFT"
author: "Noam Ross"
date: "1/16/2017"
output: html_document
---

## Part I: Changes in the Process

Process: 
-  Multiple handling editors
-  Increased use of automation - ropensci-bot
-  Commitment to multiple reviewers
-  Increased use of automated tools upstream before assigning to reviewers
    - goodpractice, covr, spellcheck
-  Checklists for editors, reviewers, and reviewees
-  Linkage with JOSS
Policies:
-  Overlap requirements
-  Generalizability
-   Data packages, data extraction packages (linked to our expansion of PDF and OCR tools)

Different requirements:
-  CI and coverage required
   - Tools have gotten better, tutorials more available
-  Awknowledging reviewers
-  Little things
   - Function naming
   - First-point-of-entry documentation

## Part II: The Numbers
*Here are some initial stabs at a quantitative look at the review process,
which will be part of my next "Year in Reviews" post.  I'll update with nicer
styling and more text later.  For now this is just an exploration to prompt further
questions. Please let me know if there's something else you want.*

```{r setup, include=FALSE}
library(airtabler)
library(tidyverse)
library(gh)
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```

```{r get-airtable}
baseID = "appZIB8hgtvjoV99D"
reviewers <- air_get(baseID, "Reviewers") %>% as_tibble()
reviews <- air_get(baseID, "Reviews") %>% as_tibble()
```

We've grown the reviewer pool considerably.  Here's the number of reviewers
by the reviews they've started and completed:

```{r reviewer-pool}
reviewers = reviewers %>% 
  mutate(n_reviews = map_int(Reviews, length)) %>% 
  mutate(last_review = map_chr(Reviews, function(x) {
    rev_reviews_urls = na.omit(filter(reviews, id %in% unlist(x))[["onboarding_url"]])
    if(!length(rev_reviews_urls)) return(NA)
    rev_reviews = map(stringi::stri_extract_first_regex(rev_reviews_urls, "(?<=https://github.com/).*$"),
                      function(x) {
                        gh_issue = gh(paste0("/repos/", x), .limit=200)
                        closed = gh_issue[["closed_at"]]
                        if(is.null(closed)) {
                          closed = as.character(Sys.Date())
                        }
                        return(closed)
                        })
    last_date = max(as.POSIXct(unlist(rev_reviews)), na.rm=TRUE)
    return(as.character(last_date))
  })) %>% 
  mutate(last_review = as.POSIXct(last_review)) %>% 
  mutate(status = if_else(is.na(last_review), "Never reviewed",
                          if_else(last_review >= as.POSIXct(Sys.Date() - 1), "Currently reviewing", "Has reviewed"))) %>% 
  mutate(last_review = coalesce(last_review, as.POSIXct("2015-01-01")))
ggplot(reviewers, aes(x=n_reviews)) + geom_bar()
```

And here they are by the date of the end of their last review, with the cutoff
of 6 months shown.  The spike at the beginning is those with no reviews, the spike
at today is those currently reviewing a package.  This is conservative as it
includes packages that are in "holding" mode, with no current expectation as to
when authors will respons to reviews.

We currently have `r nrow(reviewers)` reviewers in the database, and `r sum(reviewers$last_review < as.POSIXct(Sys.Date() - lubridate::days(180)))` have
not done a review in at least six months. `r sum(reviewers$status == "Never reviewed")`
of these are new and have not yet started a review.  We're not even including
here package authors that we have not yet tapped for review, as they are not
yet in the reviewer database.

```{r by-time}
ggplot(reviewers, aes(x=last_review, fill=as.factor(n_reviews))) +
  geom_histogram() +
  geom_vline(xintercept = as.numeric(as.POSIXct(Sys.Date() - lubridate::days(180))))
```

Here is the distribution of review times. Mean review time is ~77 days, with a
long tail but also some being much faster.  This isn't bad considering in an
absolutely perfect scenario:

- 1 week with editor
- 1 week finding reviewers
- 3 weeks reviewing
- 2 weeks revising
- 1 week getting second reviews
- = 8 weeks = 56 days

```{r time-to-accept}
packages <- gh("/repos/ropensci/onboarding/issues?labels=6/approved&state=closed", .limit=200)
times <- map_df(packages, function(x) {
  data_frame(opened = x$created_at, closed = x$closed_at)
}) %>% 
  mutate_all(funs(as.POSIXct)) %>% 
  mutate(review_time = round(as.numeric(difftime(closed, opened, units="weeks")/4)))
ggplot(times, aes(x=review_time)) + 
  #geom_histogram(binwidth = 1) +
  #geom_vline(xintercept = mean(times$review_time))
    geom_histogram(binwidth = 1, col="white") +
  geom_text(data=data_frame(x=0:7, y=0.25), aes(x=x, y=y, label=x), col="white", family="Avenir", size=6) +
  #scale_x_continuous(breaks=0:10) +
  #scale_y_continuous(breaks=seq(0,14, by=2), limits=c(0,14)) +
  theme_minimal() + theme(axis.text = element_blank(), panel.grid.minor = element_blank(),
                          panel.grid.major = element_blank(), axis.title = element_blank(), text = element_text(family="Avenir", size = 12))
```

Overall, review times are holding steady or going down:

```{r times}
ggplot(times, aes(x=opened, y=review_time)) + geom_point() + geom_smooth(method="lm")
```

Here's the number of pacakges submitted per quarter, highlighting authors with
more than one submittal.  Note that after a flurry
of submissions over last spring and summer, partially driven by our power users,
we're back down to lower rates of submission.  

It's possible that the surge is seasonal - it coincides with summertime for
many universities.  It also followed [last year's rOpenSci unconference](http://unconf16.ropensci.org/) and the surge of publicity from
last time we did a review.

```{r submission-rate}
submittals <- gh("/repos/ropensci/onboarding/issues?labels=package&state=all&sort=created", .limit=200)
submits <- map_df(submittals, function(x) {
  data_frame(submitted = x$created_at,
             name = stringi::stri_extract_first_regex(x$body, "(?<=Package:\\s{0,5}).*\\b"),
             author = x$user$login,
             editor = ifelse(!is.null(x$assignee$login), x$assignee$login, NA_character_))
}) %>% 
  mutate(submitted = as.POSIXct(submitted))

top_authors <- submits %>% count(author) %>% arrange(desc(n)) %>% filter(n  > 1)
submits = submits %>% 
  mutate(Author = if_else(author %in% top_authors$author, author, "other")) %>% 
  mutate(Author = forcats::fct_relevel(Author, c("other", rev(top_authors$author))))

ggplot(submits, aes(x=submitted, fill = Author)) +
  geom_histogram(binwidth = as.numeric(lubridate::days(90))) +
  scale_x_datetime(date_breaks = "90 days", date_labels = "%b %y") +
  scale_y_continuous(breaks = seq(0, 16, by=2), limits = c(0, 13), expand = c(0,0)) +
  scale_fill_brewer(type = "qual") +
  theme(axis.text.x = element_text(angle = 45, margin =margin(t=10)))

ggplot(submits, aes(x=submitted, fill = editor)) +
  geom_histogram(binwidth = as.numeric(lubridate::days(90))) +
  scale_x_datetime(date_breaks = "3 months", date_labels = "%b %y") +
  scale_y_continuous(breaks = seq(0, 16, by=2), limits = c(0, 13), expand = c(0,0)) +
  scale_fill_brewer(type = "qual") +
  theme(axis.text.x = element_text(angle = 45, margin =margin(t=10)))
```

Overall, this analysis sort of confirms my priors - we've put a good deal of
effort into improving our process and expanding our reviewer pool, but we've
lost some momentum in increasing rates of submission and expanding our author
base.  This suggests we should be finding ways to do more outreach for new
authors.

The reported time it takes to review remains similar.  The median time reported 
to complete a review is `r median(reviews$review_hours, na.rm=TRUE)` hours.

```{r review-hours}
ggplot(reviews, aes(x=round(review_hours))) +
  geom_histogram(binwidth = 1, col="white") +
  geom_text(data=data_frame(x=0:14, y=0.5), aes(x=x, y=y, label=x), col="white", family="Avenir", size=6) +
  #scale_x_continuous(breaks=0:10) +
  #scale_y_continuous(breaks=seq(0,14, by=2), limits=c(0,14)) +
  theme_minimal() + theme(axis.text = element_blank(), panel.grid.minor = element_blank(),
                          panel.grid.major = element_blank(), axis.title = element_blank(), text = element_text(family="Avenir", size = 12))
```

More potential analysis:

- Number of reviewers per package over time.

# Part III: What's next

-   More automation!
-   Outreach to other groups.  If you run or hope to run a similar group, we'd
    be glad to work with you to adopt some or all of our process
-   Scaling down to analyses, not just packages.  
